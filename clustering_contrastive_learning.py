"""
Author: Alex (Tai-Jung) Chen

Use contrastive learning for imbalance data classification (multi-class with pseudo labels generated by clustering).
"""
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from imblearn.metrics import specificity_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.manifold import TSNE
from sklearn.mixture import GaussianMixture
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score, confusion_matrix, classification_report, \
    cohen_kappa_score, precision_score, recall_score
from pytorch_metric_learning.losses import NTXentLoss, SupConLoss
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
import xgboost as xgb


def main(dataset: str, models: list, clus_algo: object, verbose: bool = False):
    """
    Train models on the embeddings learned by contrastive learning with pseudo multi-class labels generated by
    clustering algorithms.

    :param dataset: dataset
    :param models: list of models given to be trained on the embeddings
    :param clus_algo: clustering algorithm
    :param verbose: print out the confusion matrix and classification metrics if set to true
    """
    if dataset == "MPMC":
        df = pd.read_csv("datasets/preprocessed/mpmc.csv")
        df = df[df['failure.type'] != 5]
        X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-2], df['target'], test_size=0.3,
                                                            stratify=df['failure.type'], random_state=0)
    elif dataset == "FSP":
        df = pd.read_csv("datasets/preprocessed/faults.csv")
        df = df[(df['failure.type'] == 0) | (df['failure.type'] == 4) | (df['failure.type'] == 5) | (df['failure.type'] == 6)]

        # make the labels continuous
        label_mapping = {0: 0, 4: 1, 5: 2, 6: 3}
        df['failure.type'] = df['failure.type'].map(label_mapping)

        X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-2], df['target'], test_size=0.3,
                                                                        stratify=df['failure.type'], random_state=0)
    else:
        raise Exception("Invalid dataset.")

    # converting pd to np
    X_train = X_train.to_numpy()
    y_train = y_train.to_numpy()
    X_test = X_test.to_numpy()
    y_test = y_test.to_numpy()
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # clustering
    y_train_multi, maj_idx = cluster(X_train, y_train, clus_algo)
    # y_train_multi, maj_idx = y_train.copy(), 1

    # t-sne plot before contrastive learning
    tsne_plot(X_train, y_train_multi)
    tsne_plot(X_test, y_test)

    # Convert data to PyTorch tensors
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    print(f'device used to train model: {device}')

    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)
    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)
    y_train_tensor = torch.tensor(y_train_multi, dtype=torch.long).to(device)
    y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)

    # Define a simple MLP as an encoder
    class MLPEncoder(nn.Module):
        def __init__(self, input_dim, hidden_dim=128, output_dim=32):
            super(MLPEncoder, self).__init__()
            self.fc1 = nn.Linear(input_dim, hidden_dim)
            self.fc2 = nn.Linear(hidden_dim, 64)
            self.fc3 = nn.Linear(64, output_dim)

        def forward(self, x):
            x = F.relu(self.fc1(x))
            x = F.relu(self.fc2(x))
            x = self.fc3(x)
            return x

    encoder = MLPEncoder(input_dim=X_train.shape[1]).to(device)

    if dataset == "MPMC":
        optimizer = optim.Adam(encoder.parameters(), lr=0.001)
        epochs = 500
    elif dataset == "FSP":
        optimizer = optim.SGD(encoder.parameters(), lr=0.001, momentum=0.9)
        epochs = 2000
    elif dataset == "HMEQ": # to be tuned
        optimizer = optim.Adam(encoder.parameters(), lr=0.001)
        epochs = 500
    else:
        raise Exception("Invalid dataset.")

    # Train the encoder with contrastive loss 2100
    for epoch in range(epochs):
        encoder.train()
        optimizer.zero_grad()
        embeddings = encoder(X_train_tensor)

        # loss_func = NTXentLoss(temperature=0.5)
        loss_func = SupConLoss()
        loss = loss_func(embeddings, y_train_tensor)

        loss.backward()
        optimizer.step()

        if epoch % 10 == 0:
            print(f"Epoch {epoch}, Loss: {loss.item():.4f}")

    # Extract learned representations
    encoder.eval()
    X_train_embedded = encoder(X_train_tensor).detach().cpu().numpy()
    X_test_embedded = encoder(X_test_tensor).detach().cpu().numpy()
    tsne_plot(X_train_embedded, y_train_multi, epochs=epochs, optimizer=str(optimizer).split("(")[0])
    tsne_plot(X_test_embedded, y_test, epochs=epochs, optimizer=str(optimizer).split("(")[0])

    # Train models on the learned embeddings
    record_metrics = ['model', 'method', 'f1', 'precision', 'recall', 'bacc', 'kappa', 'acc', 'specificity']
    metrics = {key: [] for key in record_metrics}

    for model in models:
        model.fit(X_train_embedded, y_train)
        y_pred = model.predict(X_test_embedded)

        # convert to binary
        y_test_bin = np.where(y_test > 0, 1, 0)
        y_pred_bin = np.where(y_pred > 0, 1, 0)

        # Evaluate performance
        if verbose:
            print(f"\nmodel: {model}")
            print(confusion_matrix(y_test_bin, y_pred_bin, labels=[0, 1]))
            print(classification_report(y_test_bin, y_pred_bin))
            print(f"f1 score: {round(f1_score(y_test_bin, y_pred_bin), 4)}")
            print(f"balanced accuracy: {round(balanced_accuracy_score(y_test_bin, y_pred_bin), 4)}")

        metrics['acc'].append(round(accuracy_score(y_test_bin, y_pred_bin), 4))
        metrics['kappa'].append(round(cohen_kappa_score(y_test_bin, y_pred_bin), 4))
        metrics['bacc'].append(round(balanced_accuracy_score(y_test_bin, y_pred_bin), 4))
        metrics['precision'].append(round(precision_score(y_test_bin, y_pred_bin), 4))
        metrics['recall'].append(round(recall_score(y_test_bin, y_pred_bin), 4))
        metrics['specificity'].append(round(specificity_score(y_test_bin, y_pred_bin), 4))
        metrics['f1'].append(round(f1_score(y_test_bin, y_pred_bin), 4))

        metrics['model'].append(model)
        metrics['method'].append(f"CL")

    metrics_pd = pd.DataFrame(metrics)
    metrics_pd.to_csv(f"{dataset}_cl_{epochs}_{str(optimizer).split("(")[0]}.csv", index=False)


def tsne_plot(X: np.ndarray, y: np.ndarray, epochs: int = 0, optimizer: str = "none"):
    """
    Visualize the data via t-sne dimension reduction technique.

    :param X: feature space or embeddings
    :param y: labels
    :param epochs: number of epochs trained. default to 0 meaning the original feature space.
    :param optimizer: optimizer used to train the MLP.
    """
    # Stack into arrays
    embeddings = np.vstack(X)
    labels = np.hstack(y)

    # Apply t-SNE
    tsne = TSNE(n_components=2, perplexity=100, random_state=42)
    embeddings_2d = tsne.fit_transform(embeddings)

    # Plot
    plt.figure(figsize=(8, 6))
    sns.scatterplot(x=embeddings_2d[:, 0], y=embeddings_2d[:, 1], hue=labels, palette="bright", alpha=0.7)
    plt.title(f"t-SNE Visualization of Learned Embeddings (Epoch={epochs}; Optimizer={str(optimizer).split("(")[0]})")
    plt.legend()
    plt.show()


def cluster(X: np.ndarray, y: np.ndarray, clus: object):
    """
    This function applies the clustering algorithm on the minority class to generate minority subclasses.

    :param X: Features.
    :param y: Binary labels.
    :param clus: The clustering algorithm.
    :return: The multiclass label after applying clustering.
    """
    # X_maj = X[y == 0]
    X_mino = X[y == 1]
    y_multi = y.copy()

    y_min_multi_label = clus.fit_predict(X_mino) + 1

    # y_multi[y == 0] = y_maj_multi_label
    y_multi[y == 1] = y_min_multi_label
    return y_multi, 1


if __name__ == "__main__":
    ##### MPMC #####
    DATASET = "MPMC"
    CLUS = GaussianMixture(n_components=4, covariance_type='full')

    MODELS = [LogisticRegression(penalty='l1', solver='saga', max_iter=5000),
              GaussianNB(),
              LDA(),
              SVC(kernel='linear', C=0.1),
              SVC(kernel='rbf', C=0.5),
              DecisionTreeClassifier(random_state=42),
              RandomForestClassifier(random_state=42),
              GradientBoostingClassifier(random_state=42),
              xgb.XGBClassifier(random_state=42)]
    ##### MPMC #####

    ##### FSP #####
    # DATASET = "FSP"
    # CLUS = GaussianMixture(n_components=3, covariance_type='full')
    #
    # MODELS = [LogisticRegression(penalty='l1', solver='saga', max_iter=5000),
    #           GaussianNB(),
    #           LDA(),
    #           SVC(kernel='linear', C=0.1),
    #           SVC(kernel='rbf', C=0.5),
    #           DecisionTreeClassifier(random_state=42),
    #           RandomForestClassifier(random_state=42),
    #           GradientBoostingClassifier(random_state=42),
    #           xgb.XGBClassifier(random_state=42)]
    ##### FSP #####

    main(DATASET, MODELS, CLUS, verbose=True)

    # print("PyTorch Version:", torch.__version__)
    # print("MPS Available:", torch.backends.mps.is_available())
    # print("MPS Built:", torch.backends.mps.is_built())

    # optimizer = optim.Adam(encoder.parameters(), lr=0.001)
    # optimizer = optim.SGD(encoder.parameters(), lr=0.001, momentum=0.9)
    # optimizer = optim.RMSprop(encoder.parameters(), lr=0.001, alpha=0.99)
    # optimizer = optim.Adadelta(encoder.parameters(), lr=1.0)
